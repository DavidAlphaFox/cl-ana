tabletrans algorithm:

1. Get ultimate source tables from target-table

2. Select a source table from target-table, call it src

3. Treating every dependent reduction on src as if it were to be
   generated from src, group all dependent reductions by pass over
   src.  Logical tables need to be handled in a way that they do not
   require extra passes (i.e. they should be treated like a pass over
   the nearest physical source up the chain).

4. Get the number of passes over src required to compute the immediate
   dependent reductions of src (i.e. any reductions which literally
   loop over src).  Reductions of logical tables resulting from src
   should be included in this number of passes.  Call this number npass.

5. For the first npass passes from 3, create targets for the results
   of these npass passes.  This involves computing the context for
   each reduction from the collapsed source reductions and then
   collecting the reductions in a pass by their context; the loop
   bodies can be placed directly inside the shared context in the pass
   body.

6. Remap the result expressions for any results from the first npass
   passes so that they refer to the results returned by the targets
   from 5; one approach would be to return a list of all the pass
   results, and then just define the result target expressions to be
   calls to #'elt on the list and appropriate index.

7. Repeat 2-6 for all ultimate source tables found in step 1.

8. Repeat 1 disregarding any reductions and sources addressed by steps 1-7.

9. Repeat 1-8 until all table reductions are processed.

10. Set all logical result target statuses to t and their values to nil.

This is the basic gist of the algorithm, there are some minor details
to work through, e.g. treating physical table reductions as having a
context which comes from their own expression instead of just the
source table chain's contexts.

Also: context refers to the surrounding body of the immediate source
table's push-fields call, nesting these contexts together going up the
chain of table dependencies.  A convenient way to handle it would be
to have a function #'in-context which would take a list of ids from
which the context would be generated and a list of ids whose bodies
should be placed inside the nested context.  Technically the init
bindings and lfields will need to be handled as well, but hopefully
the basic idea is clear enough that I can reread this later.

The old algorithm did not handle merging contexts correctly, and the
physical table reduction was not implemented.  This algorithm seems to
handle both logical and physical tables in a more symmetric way.

Additionally, as for grouping the targets by pass treating them as if
they would be passes over the ultimate source tables: This may be as
simple as removing non-ultimate-source ltab and tab dependencies from
the targets and doing simple pass grouping.

--------------------------------------------------
old:

- Make lfields nicer.  table-pass, if used for the implementation,
  needs to have lfields arguments so that it can directly expand into
  executable code.  But: users should be able to safely define local
  lfields only used for a single table pass (and same for the inits).

- Fixed init bindings: pass-merge uses symbol-macrolets to bind inits
  so that each table-pass has its own private initialization bindings.

- Still could fix the lfields to be the same

+ Add pass collapsing for physical tables.

  * Algorithm: define collapse! for physical tables much like
    collapse! for logical tables.  The major difference is that
    collapse! for a given tab will need to group-by-pass (copy
    function from pass-merge) the various reductions of itself (first
    collapse tab reductions below the currently analyzed tab, just
    like the ltab collapse!) and then get the number of required
    passes for the source table.  Only for the passes which are
    present both for the source and the physical table should be
    collapsed from the physical table.
